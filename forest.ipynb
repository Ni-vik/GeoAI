{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU-Hv-09Ta0q"
      },
      "outputs": [],
      "source": [
        "# Importing necessary functions and libraries\n",
        "from datetime import date\n",
        "import typing as t\n",
        "import warnings\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import (\n",
        "    models,\n",
        "    layers,\n",
        "    applications,\n",
        "    preprocessing,\n",
        "    optimizers,\n",
        "    callbacks,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import albumentations as A\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21IIfmsBXv37",
        "outputId": "60e9f160-043b-45b8-844a-5f0fec098b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3oC8lR4YBAt",
        "outputId": "b8efb7fc-992b-45e9-bcc4-41bb73b8220c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted successfully.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/My Drive/nasa/Forest Segmented.zip'\n",
        "extracted_dir_path = '/content/'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "print(\"Files extracted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMok7n-HTbqT"
      },
      "outputs": [],
      "source": [
        "# Defining config variables\n",
        "class CONFIG:\n",
        "    root_dir_path = pathlib.Path(\n",
        "        \"/content/Forest Segmented\"\n",
        "    )\n",
        "    images_path = pathlib.Path(\n",
        "        \"/content/Forest Segmented/images\"\n",
        "    )\n",
        "    masks_path = pathlib.Path(\n",
        "        \"/content/Forest Segmented/masks\"\n",
        "    )\n",
        "    classes = [\n",
        "    ]\n",
        "    num_classes = len(classes)\n",
        "    image_size = 256\n",
        "    train_test_split = 0.8\n",
        "    verbose = 1\n",
        "    batch_size = 32\n",
        "    autotune = tf.data.AUTOTUNE\n",
        "    seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_yhIxGjYuSt"
      },
      "outputs": [],
      "source": [
        "# Defining helper function\n",
        "def prep_file(path: str, n_channels: int) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Prepares file at `path` to get an array form\n",
        "\n",
        "    Args:\n",
        "        path (str): file path\n",
        "        n_channels (int): number of channels in the image\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: image as an array form\n",
        "\n",
        "    Raises:\n",
        "        IOError: If the image or mask file cannot be loaded.\n",
        "    \"\"\"\n",
        "    # Read the images\n",
        "    file = tf.io.read_file(filename = path)\n",
        "\n",
        "    # Decode the images\n",
        "    file = tf.image.decode_jpeg(contents = file, channels = n_channels)\n",
        "\n",
        "    # Convert the image to a Tensor\n",
        "    file = tf.image.convert_image_dtype(image = file, dtype = tf.float32)\n",
        "\n",
        "    # Resize the image to the desired dimensions\n",
        "    file = tf.image.resize(images = file, size = (CONFIG.image_size, CONFIG.image_size))\n",
        "\n",
        "    # Normalize the image\n",
        "    file = tf.clip_by_value(file, clip_value_min = 0.0, clip_value_max = 1.0)\n",
        "\n",
        "    # Final conversion\n",
        "    file = tf.cast(file, dtype = tf.float32)\n",
        "\n",
        "    return file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhKhd9i9Y51x"
      },
      "outputs": [],
      "source": [
        "# Partial functions to make data prep easy\n",
        "load_and_prep_image = partial(prep_file, n_channels=3)\n",
        "load_and_prep_mask = partial(prep_file, n_channels=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr55gAwKY8MP",
        "outputId": "e7580fa6-270b-4ef2-e9e0-6046d8d1cb3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32, name=None))>,\n",
              " <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32, name=None))>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_dataloader = tf.data.Dataset.list_files(str(CONFIG.images_path) + \"/*.jpg\", shuffle=False)\n",
        "mask_dataloader = tf.data.Dataset.list_files(str(CONFIG.masks_path) + \"/*.jpg\", shuffle=False)\n",
        "\n",
        "image_dataloader = image_dataloader.map(lambda x: load_and_prep_image(x), num_parallel_calls=CONFIG.autotune)\n",
        "mask_dataloader = mask_dataloader.map(lambda x: load_and_prep_mask(x), num_parallel_calls=CONFIG.autotune)\n",
        "\n",
        "dataloader = tf.data.Dataset.zip((image_dataloader, mask_dataloader))\n",
        "num_files = dataloader.cardinality().numpy()\n",
        "\n",
        "train_size = int(CONFIG.train_test_split * num_files)\n",
        "\n",
        "train_dataloader = dataloader.take(train_size).repeat(3).shuffle(200, seed=CONFIG.seed, reshuffle_each_iteration=True).batch(CONFIG.batch_size).prefetch(1)\n",
        "test_dataloader = dataloader.skip(train_size).batch(CONFIG.batch_size).prefetch(1)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEIYDh1YY9tn"
      },
      "outputs": [],
      "source": [
        "def ConvBlock(inputs, filters=64):\n",
        "\n",
        "    # Taking first input and implementing the conv block\n",
        "    conv1 = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(inputs)\n",
        "    batch_norm1 = layers.BatchNormalization()(conv1)\n",
        "    act1 = layers.ReLU()(batch_norm1)\n",
        "\n",
        "    # Taking first input and implementing the second conv block\n",
        "    conv2 = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(act1)\n",
        "    batch_norm2 = layers.BatchNormalization()(conv2)\n",
        "    act2 = layers.ReLU()(batch_norm2)\n",
        "\n",
        "    return act2\n",
        "\n",
        "# Building the encoder\n",
        "def encoder(inputs, filters=64):\n",
        "\n",
        "    # Collect the start and end of each sub-block for normal pass and skip connections\n",
        "    enc1 = ConvBlock(inputs, filters)\n",
        "    max_pool1 = layers.MaxPool2D(strides=(2, 2))(enc1)\n",
        "    return enc1, max_pool1\n",
        "\n",
        "# Building the decoder\n",
        "def decoder(inputs, skip,filters):\n",
        "\n",
        "    # Upsampling and concatenating the essential features\n",
        "    upsample = layers.Conv2DTranspose(filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    connect_skip = layers.Concatenate()([upsample, skip])\n",
        "    out = ConvBlock(connect_skip, filters)\n",
        "    return out\n",
        "\n",
        "# Building the model\n",
        "def U_Net(image_size):\n",
        "    inputs = layers.Input(image_size)\n",
        "\n",
        "    # Constructing the encoder blocks and increasing the filters by a factor of 2\n",
        "    skip1, encoder_1 = encoder(inputs, 64)\n",
        "    skip2, encoder_2 = encoder(encoder_1, 64*2)\n",
        "    skip3, encoder_3 = encoder(encoder_2, 64*4)\n",
        "    skip4, encoder_4 = encoder(encoder_3, 64*8)\n",
        "\n",
        "    # Preparing the next block\n",
        "    conv_block = ConvBlock(encoder_4, 64*16)\n",
        "\n",
        "    # Constructing the decoder blocks and decreasing the filters by a factor of 2\n",
        "    decoder_1 = decoder(conv_block, skip4, 64*8)\n",
        "    decoder_2 = decoder(decoder_1, skip3, 64*4)\n",
        "    decoder_3 = decoder(decoder_2, skip2, 64*2)\n",
        "    decoder_4 = decoder(decoder_3, skip1, 64)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(decoder_4)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNRpWKeeZF7P"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of the model\n",
        "INPUT_SHAPE = (256, 256, 3)\n",
        "model = U_Net(INPUT_SHAPE)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\")\n",
        "\n",
        "# Create callbacks\n",
        "CALLBACKS = [\n",
        "    callbacks.ModelCheckpoint(monitor=\"val_loss\", verbose=1,filepath=\"U_Net_Forest_Segmentation.h5\", save_best_only=True, save_weights_only=False),\n",
        "    callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNopNKNOZIuU",
        "outputId": "084205bb-f406-4571-f495-ef9b95e32930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4990\n",
            "Epoch 1: val_loss improved from inf to 5.57495, saving model to U_Net_Forest_Segmentation.h5\n",
            "384/384 [==============================] - 795s 2s/step - loss: 0.4990 - val_loss: 5.5750\n",
            "Epoch 2/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4539\n",
            "Epoch 2: val_loss improved from 5.57495 to 0.72771, saving model to U_Net_Forest_Segmentation.h5\n",
            "384/384 [==============================] - 706s 2s/step - loss: 0.4539 - val_loss: 0.7277\n",
            "Epoch 3/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4442\n",
            "Epoch 3: val_loss improved from 0.72771 to 0.58074, saving model to U_Net_Forest_Segmentation.h5\n",
            "384/384 [==============================] - 720s 2s/step - loss: 0.4442 - val_loss: 0.5807\n",
            "Epoch 4/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4341\n",
            "Epoch 4: val_loss did not improve from 0.58074\n",
            "384/384 [==============================] - 700s 2s/step - loss: 0.4341 - val_loss: 0.5959\n",
            "Epoch 5/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4292\n",
            "Epoch 5: val_loss improved from 0.58074 to 0.46700, saving model to U_Net_Forest_Segmentation.h5\n",
            "384/384 [==============================] - 709s 2s/step - loss: 0.4292 - val_loss: 0.4670\n",
            "Epoch 6/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4210\n",
            "Epoch 6: val_loss did not improve from 0.46700\n",
            "384/384 [==============================] - 704s 2s/step - loss: 0.4210 - val_loss: 1.1397\n",
            "Epoch 7/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4166\n",
            "Epoch 7: val_loss did not improve from 0.46700\n",
            "384/384 [==============================] - 703s 2s/step - loss: 0.4166 - val_loss: 1.8643\n",
            "Epoch 8/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4110\n",
            "Epoch 8: val_loss did not improve from 0.46700\n",
            "384/384 [==============================] - 703s 2s/step - loss: 0.4110 - val_loss: 4.2857\n",
            "Epoch 9/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4093\n",
            "Epoch 9: val_loss did not improve from 0.46700\n",
            "384/384 [==============================] - 723s 2s/step - loss: 0.4093 - val_loss: 0.4777\n",
            "Epoch 10/10\n",
            "384/384 [==============================] - ETA: 0s - loss: 0.4073\n",
            "Epoch 10: val_loss improved from 0.46700 to 0.44246, saving model to U_Net_Forest_Segmentation.h5\n",
            "384/384 [==============================] - 705s 2s/step - loss: 0.4073 - val_loss: 0.4425\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "history_u_net = model.fit(train_dataloader,\n",
        "                          epochs=NUM_EPOCHS,\n",
        "                          validation_data=test_dataloader,\n",
        "                          callbacks=CALLBACKS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PuVDfTkUbNOP"
      },
      "outputs": [],
      "source": [
        "model = models.load_model(\"/content/U_Net_Forest_Segmentation.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDvElKOYDirt"
      },
      "outputs": [],
      "source": [
        "# Inferencing on test data\n",
        "test_samples = test_dataloader.shuffle(200).take(1)\n",
        "test_samples_preds = model.predict(test_samples, verbose=0)\n",
        "test_samples_preds[test_samples_preds >= 0.5] = 1\n",
        "test_samples_preds[test_samples_preds < 0.5] = 0\n",
        "\n",
        "for images, masks in test_samples:\n",
        "    for image, mask, preds in zip(images[:3], masks[:3], test_samples_preds[:3]):\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(image)\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(mask, cmap='gray')\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(preds, cmap='gray')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCeLtusdFSh9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}